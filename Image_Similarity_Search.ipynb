{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6314b507-7875-4eff-a840-829558941073",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Import Required Libraries</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec9d4a4-42d7-42d4-99e9-7bd3b01aa4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some require library\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import os\n",
    "os.environ['SSL_CERT_FILE'] = '/etc/ssl/certs/ca-certificates.crt'\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7329503",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Customize Image Dataset</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cbdb50-d341-44d8-a942-7e361236523b",
   "metadata": {},
   "source": [
    "## CustomImageDataset Class\n",
    "\n",
    "Represents a custom dataset of images. Each method collectively handles dataset initialization, data access, and feature vector processing, providing core functionalities required for working with the CustomImageDataset class.\n",
    "\n",
    " `__init__(self, root_dir, transform=None, include_feature_vector=True)`\n",
    "\n",
    "- Initializes the dataset with the root directory where images are stored, an optional transformation to be applied to the images, and a boolean flag to include pre-computed feature vectors.\n",
    "\n",
    " `__len__(self)`\n",
    "\n",
    "- Returns the total number of images in the dataset.\n",
    "\n",
    "`__getitem__(self, idx)`\n",
    "\n",
    "- Retrieves an item (image and its associated label) from the dataset given its index. Returns the image name, image data, and optionally pre-computed feature vector.\n",
    "\n",
    "`_generate_image_list(self)`\n",
    "\n",
    "- Creates a list of image paths and their associated class labels.\n",
    "\n",
    "`process_images_and_save_vectors(self, get_vector_func)`\n",
    "\n",
    "- Processes images and saves their feature vectors. Accepts a function to compute feature vectors for images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058b3c5e-95a7-4331-9891-6c6c63bc245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a class named CustomImageDataset that inherits from PyTorch's Dataset class.\n",
    "class CustomImageDataset(Dataset):\n",
    "\n",
    "    #Initializes the dataset with the root directory where images are stored (root_dir), an optional transformation to be applied to the images (transform), and a boolean flag include_feature_vector which determines whether to include pre-computed feature vectors for the images.\n",
    "    def __init__(self, root_dir, transform=None, include_feature_vector=True):\n",
    "        # Stores the root directory where the images are located.\n",
    "        self.root_dir = root_dir\n",
    "        # Stores the transformation function to be applied to the images (e.g., resizing, cropping).\n",
    "        self.transform = transform\n",
    "        # Contains a sorted list of class names, derived from the subdirectories in the root directory.\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        # Contains a list of tuples, where each tuple consists of a class label and the path to an image file. This list is generated by the _generate_image_list() method.\n",
    "        self.image_list = self._generate_image_list()\n",
    "        # Stores a boolean flag indicating whether to include pre-computed feature vectors for the images.\n",
    "        self.include_feature_vector = include_feature_vector\n",
    "\n",
    "    # returns the total number of images in the dataset.\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    # retrieves an item (image and its associated label) from the dataset given its index idx.\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        #1. Extracts the label and image path for the specified index from the image_list.\n",
    "        label, image_path = self.image_list[idx]\n",
    "        image_name = os.path.basename(os.path.dirname(image_path))  # Get subcategory name\n",
    "        \n",
    "        #2. Use PIL to open the image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        #3. Applies the specified transformation (transform) to the image, if provided.\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        #4. Initializes a default feature vector. This will be replaced if include_feature_vector is set to True.\n",
    "        feature_vector = np.array([[1, 1, 1]])\n",
    "        \n",
    "        if self.include_feature_vector:\n",
    "            #5. Generates the filename and path for the feature vector file associated with the current image\n",
    "            vector_name = f'{os.path.splitext(os.path.basename(image_path))[0]}_vector.npy'\n",
    "            vector_path = os.path.join(os.path.dirname(image_path), vector_name)\n",
    "            #6. Loads the pre-computed feature vector from the corresponding .npy file.\n",
    "            feature_vector = np.load(vector_path)\n",
    "\n",
    "        return label, image_name, image, feature_vector\n",
    "\n",
    "    # create a list of image paths and their associated class labels.\n",
    "    def _generate_image_list(self):\n",
    "        image_list = []\n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(self.root_dir, class_name)\n",
    "            for root, _, files in os.walk(class_path):\n",
    "                for file in files:\n",
    "                    if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        image_list.append((class_name, os.path.join(root, file)))\n",
    "        return image_list\n",
    "\n",
    "    # process images and save their feature vectors.\n",
    "    def process_images_and_save_vectors(self, get_vector_func, model, layer, output_size):\n",
    "        # Iterate over each image in the dataset.\n",
    "        for idx in range(len(self)):\n",
    "            # Get the label and image path for the current index.\n",
    "            label, _, _, _ = self.__getitem__(idx)\n",
    "            # Extract the image path from image_list for the current index.\n",
    "            img_path = self.image_list[idx][1]\n",
    "            # Compute the feature vector for the current image.\n",
    "            image_feature = get_vector_func(img_path, model, layer, output_size)\n",
    "            # Save the image feature vector in the same directory with .npy extension\n",
    "            save_path = os.path.splitext(img_path)[0] + '_vector.npy'\n",
    "            np.save(save_path, image_feature)\n",
    "        print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b7546-bc5e-46d1-96a2-09ac881f410e",
   "metadata": {},
   "source": [
    "### Dataset & DataLoader\n",
    "\n",
    "- **Dataset**: The dataset is the entire collection of data samples, representing the raw data. It provides an interface to access individual samples and their corresponding labels or features. In the context of PyTorch, a dataset typically encapsulates functionalities such as loading data from disk, applying transformations, and providing access to individual samples.\n",
    "\n",
    "- **DataLoader**: The DataLoader is a utility that wraps around the dataset, providing efficient data loading during training or evaluation. It samples mini-batches from the dataset, allowing you to iterate over these mini-batches. The DataLoader handles tasks such as batching, shuffling, and parallelizing data loading, which are crucial for improving training efficiency and preventing bottlenecks.\n",
    "\n",
    "So, while the dataset contains the entire dataset, the DataLoader provides a mechanism to sample batches of data from the dataset, making it easier to train models in mini-batch fashion, which is typical in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739a544-5c23-4cd0-abd6-5076a731e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Simple Image Transform\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Get the whole dataset \n",
    "image_directory = 'Fashion_Items_Dataset/'  # Will be Replace with the actual directory path of FULL DATASET\n",
    "full_custom_dataset = CustomImageDataset(root_dir=image_directory, transform=image_transform, include_feature_vector=False)\n",
    "\n",
    "# Define the DataLoader\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(dataset=full_custom_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686917e-6937-4149-a707-2488da4af689",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Data Exploration</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895783d6-b773-48b5-b33e-efa94a904729",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_images = len(full_custom_dataset)\n",
    "print(\"Total number of images in the dataset:\", total_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df17b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the image formats and visualize as a graph\n",
    "image_formats = {}\n",
    "for _, img_path in full_custom_dataset.image_list:\n",
    "    image_format = os.path.splitext(img_path)[1].lower()\n",
    "    if image_format in image_formats:\n",
    "        image_formats[image_format] += 1\n",
    "    else:\n",
    "        image_formats[image_format] = 1\n",
    "\n",
    "formats = list(image_formats.keys())\n",
    "counts = list(image_formats.values())\n",
    "\n",
    "plt.bar(formats, counts)\n",
    "plt.xlabel('Image Format')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Display the count value on top of each bar\n",
    "for i, count in enumerate(counts):\n",
    "    plt.text(i, count, str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.title('Image Format in the Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305178b2",
   "metadata": {},
   "source": [
    "The most common image format in the dataset is **JPG**, with **14730** images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16089eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Fashion Item Categories : {full_custom_dataset.classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62426db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each category\n",
    "category_counts = {label: 0 for label in full_custom_dataset.classes}\n",
    "for label, _ in full_custom_dataset.image_list:\n",
    "    category_counts[label] += 1\n",
    "\n",
    "# Extract labels and counts for plotting\n",
    "categories = list(category_counts.keys())\n",
    "counts = list(category_counts.values())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(categories, counts)\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Each Categories')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate labels for better readability\n",
    "\n",
    "# Annotate each bar with its count\n",
    "for bar, count in zip(bars, counts):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(count),\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d22b04",
   "metadata": {},
   "source": [
    "Distribution Of Product Categories: The most popular category is **Shirts**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735cb70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch from the DataLoader\n",
    "for _, image_names, images, _ in data_loader:\n",
    "    # Extract the RGB image tensor of the first image in the batch\n",
    "    rgb_image = images[0].permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "    \n",
    "    # Display each color channel separately\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Display the Red channel\n",
    "    axs[0].imshow(rgb_image[:, :, 0], cmap='Reds')\n",
    "    axs[0].set_title('Red channel')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Display the Green channel\n",
    "    axs[1].imshow(rgb_image[:, :, 1], cmap='Greens')\n",
    "    axs[1].set_title('Green channel')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    # Display the Blue channel\n",
    "    axs[2].imshow(rgb_image[:, :, 2], cmap='Blues')\n",
    "    axs[2].set_title('Blue channel')\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    break  # Break after showing the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08dd735",
   "metadata": {},
   "source": [
    "A Pixel-Level Analysis : using the RGB (Red, Green, Blue) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1898fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first batch from the DataLoader\n",
    "for _, _, images, _ in data_loader:\n",
    "    # Extract the RGB pixel values from the first image\n",
    "    rgb_image = images[0].permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "    r_channel = rgb_image[:,:,0].flatten()\n",
    "    g_channel = rgb_image[:,:,1].flatten()\n",
    "    b_channel = rgb_image[:,:,2].flatten()\n",
    "\n",
    "    # Plot the color distribution\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axs[0].hist(r_channel, bins=50, color='red', alpha=0.5)\n",
    "    axs[0].set_title('Red Channel')\n",
    "    axs[0].set_xlabel('Pixel Intensity')\n",
    "    axs[0].set_ylabel('Frequency')\n",
    "    \n",
    "    axs[1].hist(g_channel, bins=50, color='green', alpha=0.5)\n",
    "    axs[1].set_title('Green Channel')\n",
    "    axs[1].set_xlabel('Pixel Intensity')\n",
    "    axs[1].set_ylabel('Frequency')\n",
    "\n",
    "    axs[2].hist(b_channel, bins=50, color='blue', alpha=0.5)\n",
    "    axs[2].set_title('Blue Channel')\n",
    "    axs[2].set_xlabel('Pixel Intensity')\n",
    "    axs[2].set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    break  # Break after showing the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7240a1",
   "metadata": {},
   "source": [
    "The distribution of pixel intensities suggests that the image have 3 channel contribute RED, GREEN and BLUE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baa6360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store sizes of the first 10 images\n",
    "sizes = []\n",
    "\n",
    "# Get a batch from the DataLoader\n",
    "for _, image_names, images, _ in data_loader:\n",
    "    # Extract the sizes of the first 10 images in the batch\n",
    "    for image in images[:10]:\n",
    "        sizes.append(image.shape[1:])  # Append the size of each image\n",
    "    \n",
    "    # Plot the distribution of sizes for the first 10 images\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.hist(sizes, bins=20, edgecolor='black')\n",
    "    ax.set_title('Distribution of Image Sizes')\n",
    "    ax.set_xlabel('Image Size')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    break  # Break after showing the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baadb766",
   "metadata": {},
   "source": [
    "Distribution Of Image Sizes  : the most common image size is around 224 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a077d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch from the DataLoader\n",
    "for _, image_names, images, _ in data_loader:\n",
    "    # Display 15 images with 5 columns and 3 rows\n",
    "    num_rows = 3\n",
    "    num_cols = 5\n",
    "    num_images_to_display = num_rows * num_cols\n",
    "    \n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 9))\n",
    "    for i in range(num_images_to_display):\n",
    "        # Extract the RGB image tensor\n",
    "        rgb_image = images[i].permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "        \n",
    "        # Determine the subplot position\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        \n",
    "        # Plot the image\n",
    "        axs[row, col].imshow(rgb_image)\n",
    "        axs[row, col].set_title(f\" {image_names[i]} \\n {images[i].shape[1:]}\")\n",
    "        axs[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    break  # Break after showing the first batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef4ac4",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Image Preprocessing</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ff340-39cf-4d91-a65d-1a15f20a1a02",
   "metadata": {},
   "source": [
    "- `transforms.Resize((224, 224))` resizes the images to a fixed size of 224x224 pixels.\n",
    "\n",
    "- Additional transformations for data augmentation include:\n",
    "  - `random_rotation`\n",
    "  - `color_jitter`\n",
    "  - `transforms.RandomHorizontalFlip()`\n",
    "  - `horizontal_flip`\n",
    "\n",
    "- `transforms.ToTensor()` converts the images to PyTorch tensor.\n",
    "\n",
    "- `transforms.Normalize()` normalizes the tensor images with mean and standard deviation values commonly used for pre-trained models trained on the ageet dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc0eefb-f8cc-48b9-a7c7-fa5187256494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional transformations for data augmentation\n",
    "random_rotation = transforms.RandomRotation(degrees=30)\n",
    "color_jitter = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
    "horizontal_flip = transforms.RandomHorizontalFlip()\n",
    "\n",
    "# Image transforms\n",
    "scaler = transforms.Resize((224, 224))\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af62eb-fd77-4157-8ef9-fe92abbfa603",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Feature Embedding Extraction</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751a9cfc-36dc-4187-b735-fee23ed4117b",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Baseline Pipeline for feature embedding extraction </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621f8b49-28ec-487f-968b-a413a1d723ad",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\">Initialize a Pretrained Model:</span>\n",
    "- This step involves loading a pre-trained convolutional neural network (CNN) model such as ResNet-18, ResNet-50, or VGG-16 using the torchvision library. These models are pre-trained on large image datasets to extract meaningful features from images.\n",
    "\n",
    "#### <span style=\"color:orange\">Select Layer for Feature Extraction:</span>\n",
    "- The selected layer for feature extraction is usually one of the intermediate layers in the CNN, which captures rich, high-level features of the input image. In this project, we select the `avgpool` layer, which typically follows the convolutional layers and performs average pooling on the feature maps.\n",
    "\n",
    "#### <span style=\"color:orange\">Load Image Data:</span>\n",
    "- This step involves loading an image from the file system using a library like PIL (Python Imaging Library) and preparing it for input to the neural network model. The image is typically resized to match the input size expected by the model and converted to a tensor.\n",
    "\n",
    "#### <span style=\"color:orange\">Image Transformation:</span>\n",
    "- Image transformation involves applying preprocessing steps to the image data to make it suitable for input to the neural network model. Common transformations include resizing, normalization (scaling pixel values to a standard range), and converting the image to a tensor.\n",
    "\n",
    "#### <span style=\"color:orange\">Initialize Feature Vector Placeholder:</span>\n",
    "- A placeholder for storing the extracted feature vector is initialized. This vector will hold the high-level features extracted by the selected layer of the neural network.\n",
    "\n",
    "#### <span style=\"color:orange\">Capture Selected Layer to Feature Vector:</span>\n",
    "- A function is defined to copy the output of the selected layer (in this case, `avgpool`) to the feature vector placeholder. This function is registered as a hook to the layer, allowing us to intercept its output during the forward pass of the model.\n",
    "\n",
    "#### <span style=\"color:orange\">Forward Pass and Feature Extraction:</span>\n",
    "- The transformed image is passed through the pre-trained model. During this process, the output of the selected layer (feature maps) is copied to the feature vector placeholder using the hook function.\n",
    "\n",
    "#### <span style=\"color:orange\">Remove the Hook After Feature Extraction:</span>\n",
    "- After the feature extraction process is completed, the hook that was registered to capture the output of the selected layer is removed. This step is crucial to prevent memory leaks and ensure proper functioning of the model in subsequent operations. Removing the hook releases the resources associated with it and allows the model to operate normally without any interference.\n",
    "\n",
    "#### <span style=\"color:orange\">Convert Feature Vector to NumPy:</span>\n",
    "- The feature vector, which is stored as a PyTorch tensor, is converted to a NumPy array for easier handling and compatibility with other libraries.\n",
    "\n",
    "#### <span style=\"color:orange\">Save as .npy File:</span>\n",
    "- Finally, the extracted feature vector is saved as a `.npy` file using NumPy's `save` function. This file can be easily loaded and used for various machine learning tasks such as image classification, object detection, or image retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526c1c2a-b1f2-4274-8b6e-c960c4e14228",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">Define some helper functions </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04974c-1a6c-4851-b51f-d7a9a8cb9d85",
   "metadata": {},
   "source": [
    "**get_vector(image_path, model, layer, output_size):**\n",
    "- This function takes an image path, a pre-trained neural network model, a layer within that model, and an output size as input. It preprocesses the image, passes it through the model until the specified layer, and extracts the feature vector from that layer's output.\n",
    "\n",
    "**plot_scatter(feature_vectors, labels):**\n",
    "- This function takes feature vectors and their corresponding labels as input. It uses Principal Component Analysis (PCA) to reduce the dimensionality of the feature vectors and then plots a scatter plot of the reduced features, where each point represents an image in the reduced feature space.\n",
    "\n",
    "**PCA_Visualization(image_directory):**\n",
    "- This function is meant to be used for visualizing the PCA analysis of feature vectors extracted from images. It loads images from the specified directory, extracts feature vectors using a pre-trained model, and then visualizes these feature vectors using PCA. It utilizes CustomImageDataset to load the dataset, creates a DataLoader to iterate over the dataset, extracts feature vectors, and finally plots the PCA scatter plot using the `plot_scatter` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d8f11-86c3-4da2-a7f1-ae3640587df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(image_path, model, layer, output_size, augment=True):\n",
    "    # 1. Load the image with Pillow library\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # 2. Create a PyTorch Variable with the transformed image (data augment)\n",
    "    if augment:\n",
    "      image = random_rotation(image)\n",
    "      image = horizontal_flip(image)\n",
    "      image = color_jitter(image)\n",
    "\n",
    "    # 3. Data Preprocessing\n",
    "    transformed_image = Variable(normalize(to_tensor(scaler(image))).unsqueeze(0))\n",
    "    \n",
    "    # 4. Create a vector of zeros that will hold our feature vector\n",
    "    # Number of the output size of the 'avgpool' layer \n",
    "    my_embedding = torch.zeros(output_size)\n",
    "    \n",
    "    # 5. Define a function that will copy the output of a layer\n",
    "    def copy_data(m, i, o):\n",
    "      my_embedding.copy_(o.data.reshape(o.data.size(1)))\n",
    "        \n",
    "    # 6. Attach that function to our selected layer\n",
    "    h = layer.register_forward_hook(copy_data)\n",
    "\n",
    "    # 7. Run the model on our transformed image\n",
    "    model(transformed_image)\n",
    "    \n",
    "    # 8. Detach our copy function from the layer\n",
    "    h.remove()\n",
    "    \n",
    "    # 9. Return the feature vector\n",
    "    return my_embedding.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to display a single image with image name and feature vector\n",
    "def show_image_with_info(image, label, feature_vector):\n",
    "    plt.imshow(image.permute(1, 2, 0))  # Convert from (C, H, W) to (H, W, C) for displaying with matplotlib\n",
    "    plt.title(f\"Label: {label}\\nFeature Vector: {feature_vector}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d1920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot scatter using PCA\n",
    "def plot_scatter(feature_vectors, labels):\n",
    "    \n",
    "    pca = PCA(n_components=81)\n",
    "    reduced_features = pca.fit_transform(feature_vectors)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for label in set(labels):\n",
    "        indices = [i for i, l in enumerate(labels) if l == label]\n",
    "        plt.scatter(reduced_features[indices, 0], reduced_features[indices, 1], label=label)\n",
    "\n",
    "    plt.title('Scatter Plot of Images based on Feature Vectors (PCA)')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a7bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to using data from dataloder to visualize PCA for gaining more insight on Feature Extraction using each pretrained model\n",
    "def PCA_Visualization(image_directory):\n",
    "    # Define the dataset\n",
    "    custom_dataset = CustomImageDataset(root_dir=image_directory, transform=image_transform, include_feature_vector=True)\n",
    "\n",
    "    # Define the DataLoader\n",
    "    data_loader = DataLoader(dataset=custom_dataset, batch_size=len(custom_dataset), shuffle=True)\n",
    "\n",
    "    # Get a batch from the DataLoader\n",
    "    for labels, image_names, images, feature_vectors in data_loader:\n",
    "        # Show the first image along with its label and feature vector\n",
    "        # show_image_with_info(images[0], image_names[0], feature_vectors[0])\n",
    "        break  # Break after showing the first batch\n",
    "        \n",
    "    # Plot PCA\n",
    "    plot_scatter(feature_vectors.numpy(), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a8032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Testing Data\n",
    "image_directory = 'Fashion_Items_For_Testing/'\n",
    "custom_dataset = CustomImageDataset(root_dir=image_directory, transform=image_transform, include_feature_vector=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75039a-4611-4a70-b445-1dc7473a5cc0",
   "metadata": {},
   "source": [
    "## &#x2B50; **<span style=\"color:violet\">Resnet18</span>**   <!-- Glowing star -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce44f35-9da3-4880-9f20-a0cc2adaaaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "resnet18_model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Use the model object to select the desired layer\n",
    "layer_of_resnet18 = resnet18_model._modules.get('avgpool')\n",
    "\n",
    "# Set model to evaluation mode\n",
    "resnet18_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d624e-3f3b-4e68-9401-f7c15af6d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the process_images_and_save_vectors function\n",
    "output_size = 512\n",
    "custom_dataset.process_images_and_save_vectors(get_vector, resnet18_model, layer_of_resnet18, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4f5074-035e-4ab5-b608-5c84f25c9b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call PCA_Visualization\n",
    "image_directory = 'Fashion_Items_For_Testing/'  # Replace with the actual directory path\n",
    "PCA_Visualization(image_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0761f47-741f-4eeb-baaf-832059850be3",
   "metadata": {},
   "source": [
    "## &#x2B50; **<span style=\"color:violet\">Resnet50</span>**   <!-- Glowing star -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e58ad72-a3d9-4d36-88d9-3c425ec10a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "resnet50_model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Use the model object to select the desired layer\n",
    "layer_of_resnet50 = resnet50_model._modules.get('avgpool')\n",
    "\n",
    "# Set model to evaluation mode\n",
    "resnet50_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42519d7-b6b4-45a6-88fb-dda2f74f4668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the process_images_and_save_vectors function\n",
    "output_size = 2048\n",
    "custom_dataset.process_images_and_save_vectors(get_vector, resnet50_model, layer_of_resnet50, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf963bae-db43-41b5-a5b0-c600c626fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call PCA_Visualization\n",
    "image_directory = 'Fashion_Items_For_Testing/'  # Replace with the actual directory path\n",
    "PCA_Visualization(image_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e0d174-12dc-4e38-98a3-e15b49b10a9f",
   "metadata": {},
   "source": [
    "\n",
    "## &#x2B50; **<span style=\"color:violet\">VGG16 ( In Progress )</span>**   <!-- Glowing star -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b589f8c-ac40-43c4-93ce-6060035f5b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "vgg16_model = models.vgg16(pretrained=True)\n",
    "\n",
    "# Use the model object to select the desired layer\n",
    "layer = vgg16_model._modules.get('avgpool')\n",
    "\n",
    "# Set model to evaluation mode\n",
    "vgg16_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a150a-ecd1-44cc-9bdd-1275ff463390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the process_images_and_save_vectors function\n",
    "# output_size = 4096\n",
    "# custom_dataset.process_images_and_save_vectors(get_vector, vgg16_model, layer, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a4b4b1-69b2-4555-9897-98dcb253644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call PCA_Visualization\n",
    "# image_directory = 'Fashion_Items_For_Testing/'  # Replace with the actual directory path\n",
    "# PCA_Visualization(image_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e7bad-8fb6-4115-aa1f-4a21064088b6",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">Now let's apply the best pretrained model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ad46f-9ccd-45f4-9fcf-ca8bb32e7c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "image_directory = 'Fashion_Items_For_Testing/'  # Will Be Replace with the actual directory path of FULL DATASET\n",
    "full_custom_dataset = CustomImageDataset(root_dir=image_directory, transform=image_transform, include_feature_vector=False)\n",
    "\n",
    "# Call the process_images_and_save_vectors function\n",
    "output_size = 2048\n",
    "full_custom_dataset.process_images_and_save_vectors(get_vector, resnet50_model, layer_of_resnet50, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba12d091-d12e-4b72-9d60-c746d32dfe05",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4671f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(feature_vectors.numpy(), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ecb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "image_directory = 'Fashion_Items_For_Testing/'  # Will Be Replace with the actual directory path of FULL DATASET\n",
    "full_custom_dataset = CustomImageDataset(root_dir=image_directory, transform=image_transform, include_feature_vector=True)\n",
    "\n",
    "# Define the DataLoader\n",
    "batch_size = 500\n",
    "data_loader = DataLoader(dataset=full_custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "# Get a batch from the DataLoader (assuming batch size is 1)\n",
    "for labels, image_names, images, feature_vectors in data_loader:\n",
    "    # Show the first image along with its label and feature vector\n",
    "    show_image_with_info(images[0], image_names[0], feature_vectors[0])\n",
    "    break  # Break after showing the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8ee02f-3039-4336-99d2-92bbe1eb8e3c",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Image Similarity Computing</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47db684-9b9f-4cd6-a174-f59e3bc610be",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">Loading the Query Image :</span>\n",
    "Load the query image using the provided path.\n",
    "\n",
    "### <span style=\"color:orange\">Extracting Features :</span>\n",
    "\n",
    "Use the pre-trained model (like ResNet18) to extract features from the query image. This involves passing the image through the model and obtaining the feature vector.\n",
    "\n",
    "### <span style=\"color:orange\">Nearest Neighbors Search :</span>\n",
    "\n",
    "Use the feature vectors of the dataset images and the query image's feature vector to perform a nearest neighbors search. This step finds the K images in the dataset that are most similar to the query image.\n",
    "\n",
    "### <span style=\"color:orange\">Displaying Results :</span>\n",
    "\n",
    "Display the query image and the K nearest neighbor images along with their similarity percentages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13389d84-ec51-4333-a4ec-ac11b7db68b4",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Nearest Neighbors Search Process using KNN </span> using KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daca7406-e92e-4c55-8099-202860b51704",
   "metadata": {},
   "source": [
    "**Initializing Nearest Neighbors Algorithm:**\n",
    "- Create an instance of the Nearest Neighbors algorithm (`knn`) with `K` neighbors and cosine similarity metric. Cosine similarity is often used when dealing with high-dimensional feature vectors.\n",
    "\n",
    "**Fitting Feature Vectors:**\n",
    "- Fit the Nearest Neighbors algorithm with the feature vectors of the dataset images. This step allows the algorithm to construct an efficient data structure for nearest neighbor queries.\n",
    "\n",
    "**Computing Nearest Neighbors:**\n",
    "- Use the fitted Nearest Neighbors algorithm to find the `K` nearest neighbors for the query image.\n",
    "- Pass the feature vector of the query image to the `kneighbors` method, which returns the distances and indices of the `K` nearest neighbors in the dataset.\n",
    "\n",
    "**Extracting Results:**\n",
    "- Convert the distances and indices arrays to lists for easier manipulation (`distances_list` and `indices_list`).\n",
    "- Each element in `distances_list` corresponds to the distance between the query image and its `K` nearest neighbors.\n",
    "- Each element in `indices_list` corresponds to the index of the `K` nearest neighbor in the dataset.\n",
    "\n",
    "**Visualizing Results:**\n",
    "- Once the nearest neighbors are computed, visualize the results.\n",
    "- Display the query image along with its title.\n",
    "- Create a figure to display the `K` nearest neighbor images.\n",
    "- Iterate over the `K` nearest neighbor indices and visualize each neighbor image along with its title, similarity percentage, and index.\n",
    "- Similarity percentage is computed as `(1 - distance) * 100`, where distance is the cosine distance between the query image and the neighbor image.\n",
    "\n",
    "**Displaying Plots:**\n",
    "- Use Matplotlib to display the query image and the `K` nearest neighbor images in separate plots.\n",
    "\n",
    "By following these steps, the `compute_similar_images` function effectively performs the Nearest Neighbors search to find the most similar images to the query image in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4728dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with dataset\n",
    "def compute_similar_images(query_index, feature_vectors, images, image_names, K=7):\n",
    "\n",
    "    query_vector = feature_vectors[query_index].reshape(1, -1)\n",
    "\n",
    "    knn = NearestNeighbors(n_neighbors=K, metric=\"cosine\")\n",
    "    knn.fit(feature_vectors)\n",
    "\n",
    "    distances, indices = knn.kneighbors(query_vector)\n",
    "    distances_list = distances.tolist()\n",
    "    indices_list = indices.tolist()\n",
    "\n",
    "    # Plot the query image\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(images[query_index].permute(1, 2, 0))\n",
    "    plt.title(f\"Query Image - {image_names[query_index]}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the K-nearest neighbors\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(K):\n",
    "        neighbor_index = indices_list[0][i]\n",
    "        similarity_percentage = (1 - distances_list[0][i]) * 100  \n",
    "        plt.subplot(1, K, i + 1)\n",
    "        plt.imshow(images[neighbor_index].permute(1, 2, 0))\n",
    "        plt.title(f\"Neighbor {i + 1}\\n{image_names[neighbor_index]}\\nSimilarity: {similarity_percentage:.2f}%\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    return indices_list\n",
    "\n",
    "query_index = 21\n",
    "indices_list = compute_similar_images(query_index, feature_vectors, images, image_names, K=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3cedce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_similarity(indices_list, query_index, image_names, K=7):\n",
    "    query_name = image_names[query_index]\n",
    "\n",
    "    matching_percentage = 0\n",
    "    for i, neighbor_index in enumerate(indices_list[0]):\n",
    "        neighbor_name = image_names[neighbor_index]\n",
    "        if query_name == neighbor_name:\n",
    "            matching_percentage += 1\n",
    "\n",
    "    matching_percentage = (matching_percentage / K) * 100\n",
    "    print(f\"Matching Percentage: {matching_percentage:.2f}%\")\n",
    "    return matching_percentage\n",
    "\n",
    "# Call the evaluate_similarity function\n",
    "matching_percentage = evaluate_similarity(indices_list, query_index, image_names, K=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3690c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with google data\n",
    "def compute_similar_images_outside_data(query_image_path, feature_vectors, images, image_names, model, layer, output_size, K):\n",
    "\n",
    "    query_vector = get_vector(query_image_path, model, layer, output_size, augment=False)\n",
    "\n",
    "    # Use Nearest Neighbors to find similar images\n",
    "    knn = NearestNeighbors(n_neighbors=K, metric=\"cosine\")\n",
    "    knn.fit(feature_vectors)\n",
    "\n",
    "    # Find the K-nearest neighbors for the query image\n",
    "    distances, indices = knn.kneighbors(query_vector.reshape(1, -1))  \n",
    "    distances_list = distances.tolist()\n",
    "    indices_list = indices.tolist()\n",
    "\n",
    "    # Plot the query image\n",
    "    query_image = Image.open(query_image_path)\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(query_image)\n",
    "    plt.title(f\"Query Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the K-nearest neighbors\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(K):\n",
    "        neighbor_index = indices_list[0][i]\n",
    "        similarity_percentage = (1 - distances_list[0][i]) * 100  \n",
    "        plt.subplot(1, K, i + 1)\n",
    "        plt.imshow(images[neighbor_index].permute(1, 2, 0))\n",
    "        plt.title(f\"Neighbor {i + 1}\\n{image_names[neighbor_index]}\\nSimilarity: {similarity_percentage:.2f}%\")\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682fb1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_image_path = \"Fashion_Items_Dataset/Backpacks/Backpack/Backpack02.jpg\"\n",
    "output_size = 2048\n",
    "# Find K-nearest neighbors for the provided query image\n",
    "compute_similar_images_outside_data(query_image_path, feature_vectors, images, image_names, resnet50_model, layer_of_resnet50, output_size, K=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
